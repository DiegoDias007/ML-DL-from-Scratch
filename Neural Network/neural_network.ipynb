{
 "cells": [
  {
   "cell_type": "raw",
   "id": "40f2391b-1d11-4b9b-97a8-56c3c64576c5",
   "metadata": {},
   "source": [
    "%pip install numpy\n",
    "%pip install h5py\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71fb9c0a-6276-4b21-ae54-07d2e672de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "60920fb5-8990-40a6-a739-6079c62b3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    X_train = None\n",
    "    Y_train = None\n",
    "    X_test = None\n",
    "    Y_test = None\n",
    "    \n",
    "    layer_dimensions = []\n",
    "    activation_functions = []\n",
    "    n_layers = 0\n",
    "    possible_activations = set([\"relu\", \"tanh\", \"sigmoid\"])\n",
    "    update_params = None\n",
    "    \n",
    "    \n",
    "    def __init__(self, X_train, Y_train, X_test, Y_test):\n",
    "        \"\"\"\n",
    "            arguments:\n",
    "                X_train -> training data\n",
    "                Y_train -> true output of the training data\n",
    "                X_test -> test data\n",
    "                Y_test -> true output of the test data\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        \n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "\n",
    "        # Adding the dimensions of the input layer\n",
    "        self.layer_dimensions = [X_train.shape[0]]\n",
    "        self.activation_functions = []\n",
    "        \n",
    "    def add_layer(self, n_hidden, activation_function):\n",
    "        \"\"\"\n",
    "            arguments:\n",
    "                n_hidden -> number of hidden units in the current layer\n",
    "                activation_function -> activation function of the current layer\n",
    "        \"\"\"\n",
    "        assert activation_function in self.possible_activations, \"invalid activation function\"\n",
    "        self.layer_dimensions.append(n_hidden)\n",
    "        self.activation_functions.append(activation_function)\n",
    "        \n",
    "    def fit(\n",
    "        self, number_iterations = 2500, learning_rate = 0.0001, _lambda=1, optimization_algo=\"default\", check_grad=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        arguments:\n",
    "            X, Y -> training-set\n",
    "            layer_dimensions -> python list: index i = layer_dimension ith layer\n",
    "            activation_functions -> python list: index i = activation ith layer\n",
    "            number_iterations -> how many times you want gradient descent to run\n",
    "            learning_rate -> the amount you want the parameters to be affected by the gradient update\n",
    "            _lambda -> regularization parameter\n",
    "            optimization_algo -> optimization algorithm that will be used to update the parameters\n",
    "            check_grad -> use gradient check during training to debug\n",
    "        return:\n",
    "            params -> updated params after all iterations\n",
    "            costs -> python list containing cost during training\n",
    "        \"\"\"\n",
    "        self.n_layers = len(self.activation_functions)\n",
    "        params = self.initialize_params()\n",
    "        self.update_params = UpdateParams(self.n_layers)\n",
    "        costs = []\n",
    "        for i in range(number_iterations + 1):\n",
    "            Al, caches = self.forward_propagation(self.X_train, params)\n",
    "            grads = self._backward_propagation(Al, caches, _lambda)\n",
    "\n",
    "            # Debug gradients\n",
    "            if check_grad:\n",
    "                self._gradient_check(params, grads, self.X_train, self.Y_train, _lambda)\n",
    "            \n",
    "            params = self.update_params.update(params, grads, learning_rate, optimization_algo)\n",
    "            cost = self.cost_function(Al, self.Y_train, params, _lambda)\n",
    "            costs.append(cost)\n",
    "            if (i % 100) == 0 or (i == number_iterations):\n",
    "                print(f\"Cost at iteration {i} is: {cost}\")\n",
    "    \n",
    "        return params, costs\n",
    "\n",
    "    def initialize_params(self):\n",
    "        \"\"\"\n",
    "        arguments:\n",
    "            layer_dimensions -> python list, index i = layer i\n",
    "        return:\n",
    "            params -> python dict: (\"Wl\") = matrix; (\"Bl\") = vector\n",
    "        \"\"\"\n",
    "        params = {}\n",
    "        for l in range(1, self.n_layers + 1):\n",
    "            n_input = self.layer_dimensions[l - 1]\n",
    "            n_hidden = self.layer_dimensions[l]\n",
    "            Wl = np.random.randn(n_hidden, n_input) * np.sqrt(2.0 / n_input)\n",
    "            bl = np.zeros((n_hidden, 1))\n",
    "            params[\"W\" + str(l)] = Wl\n",
    "            params[\"b\" + str(l)] = bl\n",
    "    \n",
    "            # Making sure the dimensions are right\n",
    "            assert(Wl.shape == (n_hidden, n_input))\n",
    "            assert(bl.shape == (n_hidden, 1))\n",
    "\n",
    "        return params\n",
    "    \n",
    "    def forward_propagation(self, X, params):\n",
    "        \"\"\"\n",
    "        arguments:\n",
    "            params -> python dict: (\"Wl\") = matrix; (\"Bl\") = vector\n",
    "        return:\n",
    "            A -> output for the Lth layer\n",
    "            caches -> will be used to compute backpropagation: (linear_cache, Z) for index i\n",
    "        \"\"\"\n",
    "        A = X\n",
    "        caches = []\n",
    "\n",
    "        # Making sure the input is of the correct size\n",
    "        assert(self.n_layers == len(self.activation_functions))\n",
    "\n",
    "        for l in range(self.n_layers):\n",
    "            A_prev = A\n",
    "            W = params[\"W\" + str(l + 1)]\n",
    "            b = params[\"b\" + str(l + 1)]\n",
    "            activation_function = self.activation_functions[l]\n",
    "            A, cache = self._forward_propagation_single_layer(A_prev, W, b, activation_function)\n",
    "            caches.append(cache)\n",
    "\n",
    "        return A, caches\n",
    "\n",
    "    def _forward_propagation_single_layer(self, A_prev, W, b, activation_function):\n",
    "        \"\"\"\n",
    "        arguments:\n",
    "            A_prev -> input from the previous layer\n",
    "            W, b -> params of the current layer\n",
    "            activation_function -> activation function for the current layer (\"sigmoid\", \"relu\")\n",
    "        return:\n",
    "            A -> output from the current layer\n",
    "            cache -> will be used to compute backpropagation: (linear_cache, Z)\n",
    "        \"\"\"\n",
    "        Z, linear_cache = self._linear_function(A_prev, W, b)\n",
    "        A = self._apply_activation_function(Z, activation_function)\n",
    "        cache = (linear_cache, Z)\n",
    "        return A, cache\n",
    "\n",
    "    def _linear_function(self, A_prev, W, b):\n",
    "        \"\"\"\n",
    "        arguments:\n",
    "            A_prev -> input from the previous layer\n",
    "            W, b -> params of the current layer\n",
    "        return:\n",
    "            Z -> matrix of shape: (n_hidden_layers, n_training_examples)\n",
    "            cache -> will be used to compute backpropagation: (A_prev, W, b)\n",
    "        \"\"\"\n",
    "        Z = np.matmul(W, A_prev) + b\n",
    "        cache = (A_prev, W, b)\n",
    "        return Z, cache\n",
    "\n",
    "    def _apply_activation_function(self, Z, activation_function):\n",
    "        \"\"\"\n",
    "        arguments:\n",
    "            Z -> linear output from the current layer\n",
    "            activation_function -> activation function for the current layer (\"sigmoid\", \"relu\")\n",
    "        \"\"\"\n",
    "        if activation_function == \"sigmoid\":\n",
    "            A = 1 / (1 + np.exp(np.clip(-Z, -30, 30)))\n",
    "        elif activation_function == \"relu\":\n",
    "            A = np.maximum(0, Z)\n",
    "        elif activation_function == \"tanh\":\n",
    "            A = np.tanh(Z)\n",
    "\n",
    "        return A\n",
    "\n",
    "    def _backward_propagation(self, AL, caches, _lambda):\n",
    "        \"\"\"\n",
    "        arguments:\n",
    "            AL(Y_hat) -> output for the Lth layer\n",
    "            Y -> true output from the dataset\n",
    "            caches -> will be used to compute the gradients: (linear_cache, Z) for index i\n",
    "        return:\n",
    "            grads -> gradients for each layer parameter\n",
    "        \"\"\"\n",
    "        grads = {}\n",
    "        m = AL.shape[1]\n",
    "\n",
    "        assert(self.n_layers == len(caches))\n",
    "        assert(AL.shape == self.Y_train.shape)\n",
    "\n",
    "        # Computing the gradients for the Lth layer to start out backward propagation\n",
    "        cache = caches[self.n_layers - 1]\n",
    "        linear_cache, Z = cache\n",
    "        _, W, _ = linear_cache\n",
    "        activation_function = self.activation_functions[self.n_layers - 1]\n",
    "\n",
    "        dAL = - np.divide(self.Y_train, AL) + np.divide(1 - self.Y_train, 1 - AL)\n",
    "        dA_prev, dW, db = self._backward_propagation_single_layer(dAL, cache, activation_function)\n",
    "        dW += (_lambda / m) * W\n",
    "\n",
    "        grads[\"dA\" + str(self.n_layers - 1)] = dA_prev\n",
    "        grads[\"dW\" + str(self.n_layers)] = dW\n",
    "        grads[\"db\" + str(self.n_layers)] = db\n",
    "\n",
    "        for l in reversed(range(self.n_layers - 1)):\n",
    "            cache = caches[l]\n",
    "            linear_cache, Z = cache\n",
    "            _, W, _ = linear_cache\n",
    "            activation_function = self.activation_functions[l]\n",
    "\n",
    "            dA = grads[\"dA\" + str(l + 1)]\n",
    "            dA_prev, dW, db = self._backward_propagation_single_layer(dA, cache, activation_function)\n",
    "            reg_term = (_lambda / m) * W\n",
    "            dW += reg_term\n",
    "\n",
    "            grads[\"dA\" + str(l)] = dA_prev\n",
    "            grads[\"dW\" + str(l + 1)] = dW\n",
    "            grads[\"db\" + str(l + 1)] = db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def _backward_propagation_single_layer(self, dA, cache, activation_function):\n",
    "        \"\"\"\n",
    "        arguments:\n",
    "            dA -> derivative of the cost with respect to the activation output\n",
    "            cache -> (linear_cache, Z)\n",
    "            activation_function -> activation function for the current layer (\"sigmoid\", \"relu\")\n",
    "        return:\n",
    "            dA_prev, dW, db -> derivative of the cost with respect to the following\n",
    "        \"\"\"\n",
    "        linear_cache, Z = cache\n",
    "        dZ = self._activation_backward(dA, Z, activation_function)\n",
    "        dA_prev, dW, db = self._linear_backward(dZ, linear_cache)\n",
    "        return dA_prev, dW, db\n",
    "\n",
    "    def _activation_backward(self, dA, Z, activation_function):\n",
    "        \"\"\"\n",
    "        arguments:\n",
    "            dA -> derivative of the cost function with respect to A(l)\n",
    "            Z -> linear output of the current layer\n",
    "            activation_function -> activation function for the current layer (\"sigmoid\", \"relu\")\n",
    "        return:\n",
    "            dZ -> derivative of the cost function with respect to Z(l)\n",
    "        \"\"\"\n",
    "        A = self._apply_activation_function(Z, activation_function)\n",
    "        if activation_function == \"sigmoid\":\n",
    "            dZ = dA * A * (1 - A)\n",
    "        elif activation_function == \"relu\":\n",
    "            dZ = np.array(dA, copy=True)\n",
    "            dZ[Z <= 0] = 0\n",
    "        elif activation_function == \"tanh\":\n",
    "            dZ = dA * (1 - np.square(A))\n",
    "\n",
    "        return dZ\n",
    "\n",
    "    def _linear_backward(self, dZ, linear_cache):\n",
    "        \"\"\"\n",
    "        arguments:\n",
    "            dZ -> derivative of the cost function with respect to Z(l)\n",
    "            linear_cache -> (A_prev, W, b)\n",
    "        return:\n",
    "            dA_prev, dW, db -> derivatives of the cost function with respect to each one\n",
    "        \"\"\"\n",
    "        A_prev, W, b = linear_cache\n",
    "        num_neurons_current_layer, num_examples = dZ.shape\n",
    "        num_neurons_prev_layer = A_prev.shape[0]\n",
    "\n",
    "        # Calculate the gradients\n",
    "        dA_prev = np.matmul(W.T, dZ)\n",
    "        dW = (1 / num_examples) * np.matmul(dZ, A_prev.T)\n",
    "        db = (1 / num_examples) * np.sum(dZ, axis=1).reshape(-1, 1)\n",
    "\n",
    "        # Making sure the dimensions are right\n",
    "        assert(dA_prev.shape == (num_neurons_prev_layer, num_examples))\n",
    "        assert(dW.shape == (num_neurons_current_layer, num_neurons_prev_layer))\n",
    "        assert(db.shape == (num_neurons_current_layer, 1))\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "\n",
    "    def cost_function(self, Al, Y, params, _lambda):\n",
    "        m = Y.shape[1]\n",
    "        loss = Y * np.log(Al) + (1 - Y) * np.log(1 - Al)\n",
    "        cost = (-1 / m) * np.sum(loss)\n",
    "    \n",
    "        # Compute regularization term\n",
    "        L2_regularization_term = 0\n",
    "        for key in params.keys():\n",
    "            if 'W' in key:\n",
    "                L2_regularization_term += np.sum(np.square(params[key]))\n",
    "    \n",
    "        cost += (_lambda / (2 * m)) * L2_regularization_term\n",
    "        return cost\n",
    "\n",
    "    def evaluate(self, AL, Y):\n",
    "        n = Y.shape[1]\n",
    "        AL_binary = [0] * n\n",
    "        for i in range(n):\n",
    "            if AL[0][i] > 0.5:\n",
    "                AL_binary[i] = 1\n",
    "            else:\n",
    "                AL_binary[i] = 0\n",
    "                \n",
    "        correct_pred = np.equal(AL_binary, Y)\n",
    "        correct = 0\n",
    "        for i in range(n):\n",
    "            correct += correct_pred[0][i]\n",
    "        \n",
    "        correct_percentage = (correct / n) * 100\n",
    "        return correct_percentage\n",
    "\n",
    "\n",
    "    # Used to debug the gradient computation\n",
    "    def _gradient_check(self, parameters, gradients, X, Y, _lambda, epsilon=1e-7):\n",
    "        \"\"\"\n",
    "        Checks if backward propagation computes the correct gradient for each parameter.\n",
    "        \n",
    "        Arguments:\n",
    "        parameters -- dictionary containing the parameters \"W1\", \"b1\", ..., \"WL\", \"bL\"\n",
    "        gradients -- dictionary containing the gradients computed by backprop\n",
    "        X -- input data of shape (input size, number of examples)\n",
    "        Y -- true \"label\"\n",
    "        epsilon -- tiny shift to the input to compute approximated gradient\n",
    "    \n",
    "        Returns:\n",
    "        difference -- relative difference between the approximated gradient and the backprop gradient\n",
    "        \"\"\"\n",
    "        gradapprox = {}\n",
    "        for key in parameters.keys():\n",
    "            gradapprox[\"d\" + key] = np.zeros_like(parameters[key])\n",
    "            # Loop through each element of the parameter matrix\n",
    "            for i in range(parameters[key].shape[0]):\n",
    "                for j in range(parameters[key].shape[1]):\n",
    "                    # Compute J_plus by slightly increasing the parameter\n",
    "                    parameters[key][i, j] += epsilon\n",
    "                    AL, _ = self.forward_propagation(X, parameters)\n",
    "                    J_plus = self.cost_function(AL, Y, parameters, _lambda)\n",
    "                    \n",
    "                    # Compute J_minus by slightly decreasing the parameter\n",
    "                    parameters[key][i, j] -= 2 * epsilon\n",
    "                    AL, _ = self.forward_propagation(X, parameters)\n",
    "                    J_minus = self.cost_function(AL, Y, parameters, _lambda)\n",
    "                    \n",
    "                    # Reset the parameter value\n",
    "                    parameters[key][i, j] += epsilon\n",
    "                    \n",
    "                    # Compute the gradient approximation\n",
    "                    gradapprox[\"d\" + key][i, j] = (J_plus - J_minus) / (2 * epsilon)\n",
    "        \n",
    "        # Compute the relative difference between computed gradients and gradapprox\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "\n",
    "        for key in gradients.keys():\n",
    "            # Skip keys that are not relevant for the gradient check\n",
    "            if 'dA' in key:\n",
    "                continue\n",
    "            numerator += np.linalg.norm(gradients[key] - gradapprox[key])\n",
    "            denominator += np.linalg.norm(gradients[key]) + np.linalg.norm(gradapprox[key])\n",
    "        \n",
    "        difference = numerator / denominator\n",
    "  \n",
    "        if difference > 2e-7:\n",
    "            print(f\"There is a mistake in the backward propagation! difference = {difference}\")\n",
    "        \n",
    "        return difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a22a1690-d2aa-41ee-b261-5b3a4682f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdateParams:\n",
    "    grads_momentum = {}\n",
    "    grads_rms_prop = {}\n",
    "    n_layers = 0\n",
    "\n",
    "    def __init__(self, n_l):\n",
    "        \"\"\"\n",
    "            arguments:\n",
    "                n_l -> number of layers of the neural network\n",
    "        \"\"\"\n",
    "        self.n_layers = n_l\n",
    "        for l in range(1, self.n_layers + 1):\n",
    "            self.grads_momentum[\"dW\" + str(l)] = 0\n",
    "            self.grads_momentum[\"db\" + str(l)] = 0\n",
    "\n",
    "            self.grads_rms_prop[\"dW\" + str(l)] = 0\n",
    "            self.grads_rms_prop[\"db\" + str(l)] = 0\n",
    "\n",
    "    def update(self, params, grads, learning_rate, optimization_algo, beta=0.9, beta1=0.9, beta2=0.999):\n",
    "        self.n_layers = len(params) // 2\n",
    "        if optimization_algo == \"default\":\n",
    "            params = self._default(params, grads, learning_rate)\n",
    "        elif optimization_algo == \"momentum\":\n",
    "            params = self._momentum(params, grads, learning_rate, beta)\n",
    "        elif optimization_algo == \"rms_prop\":\n",
    "            params = self._rms_prop(params, grads, learning_rate, beta)\n",
    "        elif optimization_algo == \"adam\":\n",
    "            params = self._adam(params, grads, learning_rate, beta1, beta2)\n",
    "        elif optimization_algo == \"adopt\":\n",
    "            params = self._adopt(params, grads, learning_rate, beta1, beta2=0.9999)\n",
    "\n",
    "        return params\n",
    "\n",
    "    def _default(self, params, grads, learning_rate):\n",
    "        \n",
    "        for l in range(1, self.n_layers + 1):\n",
    "            params[\"W\" + str(l)] -= learning_rate * grads[\"dW\" + str(l)]\n",
    "            params[\"b\" + str(l)] -= learning_rate * grads[\"db\" + str(l)]\n",
    "\n",
    "        return params\n",
    "\n",
    "    def _momentum(self, params, grads, learning_rate, beta):\n",
    "        \n",
    "        for l in range(1, self.n_layers + 1):\n",
    "            mW = (beta * self.grads_momentum[\"dW\" + str(l)]) + (1 - beta) * grads[\"dW\" + str(l)]\n",
    "            mb = (beta * self.grads_momentum[\"db\" + str(l)]) + (1 - beta) * grads[\"db\" + str(l)]\n",
    "            \n",
    "            params[\"W\" + str(l)] -= learning_rate * mW\n",
    "            params[\"b\" + str(l)] -= learning_rate * mb\n",
    "            \n",
    "            self.grads_momentum[\"dW\" + str(l)] = mW\n",
    "            self.grads_momentum[\"db\" + str(l)] = mb\n",
    "            \n",
    "        return params\n",
    "\n",
    "    def _rms_prop(self, params, grads, learning_rate, beta):\n",
    "        \n",
    "        epsilon = 1e-8\n",
    "        \n",
    "        for l in range(1, self.n_layers + 1):\n",
    "            rW = (beta * self.grads_rms_prop[\"dW\" + str(l)]) + (1 - beta) * np.square(grads[\"dW\" + str(l)])\n",
    "            rb = (beta * self.grads_rms_prop[\"db\" + str(l)]) + (1 - beta) * np.square(grads[\"db\" + str(l)])\n",
    "            \n",
    "            params[\"W\" + str(l)] -= learning_rate * (grads[\"dW\" + str(l)] / np.sqrt(rW + epsilon))\n",
    "            params[\"b\" + str(l)] -= learning_rate * (grads[\"db\" + str(l)] / np.sqrt(rb + epsilon))\n",
    "\n",
    "            self.grads_rms_prop[\"dW\" + str(l)] = rW\n",
    "            self.grads_rms_prop[\"db\" + str(l)] = rb\n",
    "\n",
    "        return params\n",
    "\n",
    "    def _adam(self, params, grads, learning_rate, beta1, beta2):\n",
    "        \n",
    "        epsilon = 1e-8\n",
    "        \n",
    "        for l in range(1, self.n_layers + 1):\n",
    "            mW = (beta1 * self.grads_momentum[\"dW\" + str(l)]) + (1 - beta1) * grads[\"dW\" + str(l)]\n",
    "            mb = (beta1 * self.grads_momentum[\"db\" + str(l)]) + (1 - beta1) * grads[\"db\" + str(l)]\n",
    "            \n",
    "            rW = (beta2 * self.grads_rms_prop[\"dW\" + str(l)]) + (1 - beta2) * np.square(grads[\"dW\" + str(l)])\n",
    "            rb = (beta2 * self.grads_rms_prop[\"db\" + str(l)]) + (1 - beta2) * np.square(grads[\"db\" + str(l)])\n",
    "\n",
    "            params[\"W\" + str(l)] -= learning_rate * (mW / np.sqrt(rW + epsilon))\n",
    "            params[\"b\" + str(l)] -= learning_rate * (mb / np.sqrt(rb + epsilon))\n",
    "\n",
    "            self.grads_momentum[\"dW\" + str(l)] = mW\n",
    "            self.grads_momentum[\"db\" + str(l)] = mb\n",
    "            \n",
    "            self.grads_rms_prop[\"dW\" + str(l)] = rW\n",
    "            self.grads_rms_prop[\"db\" + str(l)] = rb\n",
    "\n",
    "        return params\n",
    "\n",
    "    def _adopt(self, params, grads, learning_rate, beta1, beta2):\n",
    "\n",
    "        epsilon = 1e-6\n",
    "        \n",
    "        for l in range(1, self.n_layers + 1):\n",
    "            mW = (beta1 * self.grads_momentum[\"dW\" + str(l)]) + (1 - beta1) * (grads[\"dW\" + str(l)] / np.sqrt(self.grads_rms_prop[\"dW\" + str(l)] + epsilon))\n",
    "            mb = (beta1 * self.grads_momentum[\"db\" + str(l)]) + (1 - beta1) * (grads[\"db\" + str(l)] / np.sqrt(self.grads_rms_prop[\"db\" + str(l)] + epsilon))\n",
    "\n",
    "            rW = (beta2 * self.grads_rms_prop[\"dW\" + str(l)]) + (1 - beta2) * np.square(grads[\"dW\" + str(l)])\n",
    "            rb = (beta2 * self.grads_rms_prop[\"db\" + str(l)]) + (1 - beta2) * np.square(grads[\"db\" + str(l)])\n",
    "                                                                                        \n",
    "            params[\"W\" + str(l)] -= learning_rate * mW\n",
    "            params[\"b\" + str(l)] -= learning_rate * mb\n",
    "\n",
    "            self.grads_momentum[\"dW\" + str(l)] = mW\n",
    "            self.grads_momentum[\"db\" + str(l)] = mb\n",
    "            \n",
    "            self.grads_rms_prop[\"dW\" + str(l)] = rW\n",
    "            self.grads_rms_prop[\"db\" + str(l)] = rb\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b18869a8-fa12-47e1-80c0-d0c86bbb4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a synthetic binary classification dataset\n",
    "X, Y = make_moons(n_samples=1000, noise=0.2, random_state=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Adjusting the dimensions\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "Y_train = Y_train.reshape(1, -1)\n",
    "Y_test = Y_test.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a36e60d6-9218-4edb-a604-72b0b3f6c640",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons_first_layer, n_training_examples = X_train.shape\n",
    "n_neurons_output_layer = Y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2a781924-b9d0-4ed8-a9d6-e999a21ae2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "096f3101-25fb-4e88-8ab1-ee292078a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_layer(20, \"relu\")\n",
    "model.add_layer(7, \"relu\")\n",
    "model.add_layer(5, \"tanh\")\n",
    "model.add_layer(n_neurons_output_layer, \"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6ebe6e15-1b83-4813-a510-652b66f09297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration 0 is: 1.0024186835439246\n",
      "Cost at iteration 100 is: 0.2088453687165766\n",
      "Cost at iteration 200 is: 0.14838134645493112\n",
      "Cost at iteration 300 is: 0.13782375892085102\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "parameters, costs = model.fit(\n",
    "    number_iterations=300, learning_rate=0.001, _lambda=2, optimization_algo=\"adopt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4ba00c9c-dfde-451f-99cb-2fd81350a5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e3301dc530>]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a9c5a951-95f2-4ca0-813a-91d05fdbbc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correct predictions Training set:  97.5\n",
      "Percentage of correct predictions Test set 95.0\n"
     ]
    }
   ],
   "source": [
    "AL_train, caches = model.forward_propagation(X_train, parameters)\n",
    "AL_test, caches = model.forward_propagation(X_test, parameters)\n",
    "correct_train = model.evaluate(AL_train, Y_train)\n",
    "correct_test = model.evaluate(AL_test, Y_test)\n",
    "print(\"Percentage of correct predictions Training set: \", correct_train)\n",
    "print(\"Percentage of correct predictions Test set\", correct_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a275cdb-4970-4ce7-9905-97540b1e6db4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The model worked as expected, achieving a good accuracy in both the training set and the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
